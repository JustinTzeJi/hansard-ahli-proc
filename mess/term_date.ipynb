{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_term_date = { 11: {'start': '21 March 2004', 'end': '13 February 2008'},\n",
    " 12: {'start': '8 March 2008', 'end': '3 April 2013'},\n",
    " 13: {'start': '5 May 2013', 'end': '7 April 2018'},\n",
    " 14: {'start': '9 May 2018', 'end': '10 October 2022'},\n",
    " 15: {'start': '19 November 2022', 'end': None}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_consecutive(a):\n",
    "    return sorted(a) == list(range(min(a), max(a) + 1))\n",
    "\n",
    "def split_consec(lst):\n",
    "    out = []\n",
    "    for _, g in groupby(enumerate(lst), lambda x: x[0] - x[1]):\n",
    "        out.append([v for _, v in g])\n",
    "    return out\n",
    "\n",
    "def gen_consec(i):\n",
    "    if i[\"consecutive\"]:\n",
    "        return [i[\"vol\"]]\n",
    "    else:\n",
    "        return split_consec(i[\"vol\"])\n",
    "\n",
    "def gen_date_set(i):\n",
    "    _start = []\n",
    "    _end = []\n",
    "    for period in i[\"vol_set\"]:\n",
    "        _start.append(ge_term_date[min(period)][\"start\"])\n",
    "        _end.append(ge_term_date[max(period)][\"end\"])\n",
    "    return pd.Series([_start, _end], index=[\"start_date\",\"end_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_1st_Malayan_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_2nd_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_3rd_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_4th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_5th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_6th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_7th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_8th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_9th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_10th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_11th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_12th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_13th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_14th_Malaysian_Parliament\",\n",
    "# \"https://en.wikipedia.org/wiki/Members_of_the_Dewan_Rakyat,_15th_Malaysian_Parliament\",\n",
    "# ]\n",
    "# ge_term_date = {}\n",
    "# for vol, url in enumerate(urls):\n",
    "#     term_start, term_end = aaa, bbb = re.sub(r\"\\[.?\\]\",\"\",pd.read_html(url)[0].iloc[6,1]).split(\"\\xa0â€“ \")\n",
    "#     ge_term_date[vol + 1] = {\"start\": term_start, \"end\": None if term_end == \"present\" else term_end}\n",
    "# ge_term_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_tempoh = pd.read_csv(\"DN - dn_w_author_id.csv\")\n",
    "dn_tempoh[\"tempoh_mula\"] = pd.to_datetime(dn_tempoh[\"tempoh_mula\"])\n",
    "dn_tempoh[\"tempoh_tamat\"] = pd.to_datetime(dn_tempoh[\"tempoh_tamat\"])\n",
    "dn_tempoh = dn_tempoh.drop_duplicates()\n",
    "dn_tempoh_min = dn_tempoh[[\"author_id\",\"vol\", \"tempoh_mula\", \"tempoh_tamat\"]].drop_duplicates()\n",
    "dn_tempoh_min_copy = dn_tempoh_min.copy()\n",
    "dn_tempoh_min = pd.merge(dn_tempoh_min[[\"author_id\",\"vol\"]], dn_tempoh_min_copy.groupby([\"author_id\",\"vol\"])[\"tempoh_mula\"].min().reset_index(), on=[\"author_id\",\"vol\"])\n",
    "dn_tempoh_min = pd.merge(dn_tempoh_min[[\"author_id\",\"vol\",\"tempoh_mula\"]], dn_tempoh_min_copy.groupby([\"author_id\",\"vol\"])[\"tempoh_tamat\"].max().reset_index(), on=[\"author_id\",\"vol\"])\n",
    "\n",
    "dn_tempoh_min = dn_tempoh_min.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_date_set_dn(i, raw_data=dn_tempoh_min):\n",
    "    _start = []\n",
    "    _end = []\n",
    "    author_id = i[\"author_id\"]\n",
    "    author_data = raw_data[raw_data[\"author_id\"]==author_id]\n",
    "    for period in i[\"vol_set\"]:\n",
    "        _start.append(author_data[author_data[\"vol\"] == min(period)][\"tempoh_mula\"].iloc[0])\n",
    "        _end.append(author_data[author_data[\"vol\"] == max(period)][\"tempoh_tamat\"].iloc[0])\n",
    "    return pd.Series([_start, _end], index=[\"start_date\",\"end_date\"])\n",
    "\n",
    "pooled_dn = dn_tempoh_min[[\"author_id\", \"vol\", \"tempoh_mula\", \"tempoh_tamat\"]].sort_values(by=\"vol\", ascending=True).groupby(\"author_id\")[\"vol\"].apply(list).reset_index()\n",
    "# dn_tempoh_min[\"consecutive\"] = \n",
    "pooled_dn[\"consecutive\"] = pooled_dn[\"vol\"].apply(are_consecutive)\n",
    "pooled_dn[\"vol_set\"] = pooled_dn[[\"vol\",\"consecutive\"]].apply(gen_consec,axis=1)\n",
    "pooled_dn[[\"start_date\",\"end_date\"]] = pooled_dn.apply(gen_date_set_dn, axis=1)\n",
    "pooled_dn = pooled_dn.explode([\"vol_set\",\"start_date\", \"end_date\"])\n",
    "\n",
    "\n",
    "dn_tempoh_remap = pd.merge(dn_tempoh[['author_id', 'vol', 'nama']].drop_duplicates(), dn_tempoh_min, on=[\"author_id\",\"vol\"])\n",
    "dn_tempoh_remap.columns = ['author_id', 'vol', 'name', 'tempoh_mula', 'tempoh_tamat']\n",
    "dn_tempoh_remap[\"name\"] = dn_tempoh_remap[\"name\"].str.upper()\n",
    "dn_tempoh_remap[dn_tempoh_remap[\"vol\"].isin([15])][\"author_id\"].nunique()\n",
    "dn_tempoh_remap[dn_tempoh_remap[\"vol\"].isin([12,13,14,15])][\"vol\"].value_counts()\n",
    "author_history = pd.read_csv(\"author_history.csv\").drop_duplicates()\n",
    "test_merge = pd.merge(author_history, dn_tempoh_remap[[\"author_id\", \"name\",\"vol\",'tempoh_mula', 'tempoh_tamat']], on=[\"name\", \"vol\"], how=\"left\")\n",
    "dn_author_id_mapping = test_merge[~test_merge[\"author_id_y\"].isna()][[\"new_uid\",\"author_id_y\"]].drop_duplicates().set_index(\"author_id_y\").to_dict()[\"new_uid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7q/vk27jw6d7_95l168mby_n6zc0000gn/T/ipykernel_33842/1022689298.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dn_tempoh_min_filt[\"new_uid\"] = dn_tempoh_min_filt[\"author_id\"].map(dn_author_id_mapping)\n"
     ]
    }
   ],
   "source": [
    "dn_tempoh_min_filt = dn_tempoh_min[dn_tempoh_min[\"vol\"].isin([12,13,14,15])]\n",
    "dn_tempoh_min_filt[\"new_uid\"] = dn_tempoh_min_filt[\"author_id\"].map(dn_author_id_mapping)\n",
    "author_hist_merge = pd.merge(author_history, dn_tempoh_min_filt[['vol', 'tempoh_mula', 'tempoh_tamat', 'new_uid']], \"left\")\n",
    "author_hist_dn = author_hist_merge[~author_hist_merge[\"tempoh_mula\"].isna()]\n",
    "dr_auth_hist = author_hist_merge[author_hist_merge[\"tempoh_mula\"].isna()][\"new_uid\"].values\n",
    "author_hist_dn = author_hist_dn[['vol', 'name', 'birth_year', 'sex', 'ethnicity', 'parti', 'negeri',\n",
    "       'kawasan', 'kod_kawasan', 'jawatan_kabinet', 'jawatan_parlimen',\n",
    "       'new_uid', 'tempoh_mula', 'tempoh_tamat']]\n",
    "author_hist_dn.columns = ['vol', 'author', 'birth_year', 'sex', 'ethnicity', 'party', 'negeri',\n",
    "       'area', 'kod_kawasan', 'exec_posts', 'service_posts',\n",
    "       'author_id', 'start_date', 'end_date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7q/vk27jw6d7_95l168mby_n6zc0000gn/T/ipykernel_33842/3842822722.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_term_fil[\"start_date\"] = pd.to_datetime(merged_term_fil[\"start_date\"])\n",
      "/var/folders/7q/vk27jw6d7_95l168mby_n6zc0000gn/T/ipykernel_33842/3842822722.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_term_fil[\"end_date\"] = pd.to_datetime(merged_term_fil[\"end_date\"])\n"
     ]
    }
   ],
   "source": [
    "ge_term_date_df = pd.DataFrame.from_dict(ge_term_date).T.reset_index(names=[\"vol\"])\n",
    "merged_term = pd.merge(author_history[author_history[\"new_uid\"].isin(dr_auth_hist)], ge_term_date_df, \"left\")\n",
    "test = merged_term.sort_values(by=\"vol\", ascending=True)[[\"new_uid\", \"vol\"]].groupby(by=\"new_uid\")[\"vol\"].apply(list).reset_index()\n",
    "test[\"consecutive\"] = test[\"vol\"].apply(are_consecutive)\n",
    "test[\"vol_set\"]=test[[\"vol\", \"consecutive\"]].apply(gen_consec,axis=1)\n",
    "test[[\"start_date\",\"end_date\"]] = test[[\"vol_set\"]].apply(gen_date_set,axis=1)\n",
    "test2 = test[[\"new_uid\", \"start_date\", \"end_date\"]].explode([\"start_date\", \"end_date\"])\n",
    "\n",
    "merged_term_fil = merged_term[['vol', 'name', 'birth_year', 'sex', 'ethnicity', 'parti', 'negeri',\n",
    "       'kawasan', 'kod_kawasan', 'jawatan_kabinet', 'jawatan_parlimen', 'new_uid', 'start', 'end']]\n",
    "merged_term_fil.columns = ['vol', 'author', 'birth_year', 'sex', 'ethnicity', 'party', 'negeri',\n",
    "       'area', 'kod_kawasan', 'exec_posts', 'service_posts',\n",
    "       'author_id', 'start_date', 'end_date']\n",
    "merged_term_fil[\"start_date\"] = pd.to_datetime(merged_term_fil[\"start_date\"])\n",
    "merged_term_fil[\"end_date\"] = pd.to_datetime(merged_term_fil[\"end_date\"])\n",
    "\n",
    "\n",
    "final_author_history = pd.concat([merged_term_fil, author_hist_dn])\n",
    "final_author_history[\"service_posts\"] = final_author_history[\"service_posts\"].replace(\"TIADA\", None)\n",
    "final_author_history[\"exec_posts\"] = final_author_history[\"exec_posts\"].replace(\"TIADA\", None)\n",
    "final_author_history.to_parquet(\"author_history_with_term.parquet\", index=False)\n",
    "final_author_history.to_csv(\"author_history_with_term.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = pd.read_csv(\"author.csv\")\n",
    "pooled_dn[\"author_id\"] = pooled_dn[\"author_id\"].map(dn_author_id_mapping)\n",
    "dn_author = pd.merge(author, pooled_dn[['author_id','start_date', 'end_date',]], how=\"left\", on=[\"author_id\"])\n",
    "dn_author_term = dn_author[~dn_author[\"start_date\"].isna()]\n",
    "# author = pd.read_csv(\"author.csv\")\n",
    "author_with_term = pd.merge(author[author[\"author_id\"].isin(dn_author[dn_author[\"start_date\"].isna()][\"author_id\"].values)], test2, \"left\", left_on=\"author_id\", right_on=\"new_uid\")\n",
    "author_with_term[\"start_date\"] = pd.to_datetime(author_with_term[\"start_date\"])\n",
    "author_with_term[\"end_date\"] = pd.to_datetime(author_with_term[\"end_date\"])\n",
    "author_with_term = author_with_term.drop(\"new_uid\", axis=1)\n",
    "\n",
    "author_term_export = pd.concat([dn_author_term, author_with_term])\n",
    "author_term_export.to_parquet(\"author_with_term.parquet\", index=False)\n",
    "author_term_export.to_csv(\"author_with_term.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
